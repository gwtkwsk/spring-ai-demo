# Spring AI Demo

## Run LLM locally

```
ollama run llama3.1:8b
```

## Run the app

```
./gradlew bootRun
```

## Chat with LLM

Use the `requests.http` file.
